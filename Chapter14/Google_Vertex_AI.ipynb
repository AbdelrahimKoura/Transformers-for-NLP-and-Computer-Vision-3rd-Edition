{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYvzs7cLR9oY"
      },
      "source": [
        "# Vertex AI\n",
        "Copyright 2024, Denis Rothman\n",
        "\n",
        "Google, as other editors advancing at full speed with LLM generative transformer models, is continually updating the available models.\n",
        "\n",
        "The models versions may not be stable. If you encounter an issue, go to Google's model versioning page to see which use.\n",
        "\n",
        "https://cloud.google.com/vertex-ai/docs/generative-ai/learn/model-versioning\n",
        "\n",
        "**March 28, 2024 note:** Due to a major Bigframes upgrade, there is a conflich with Vertex AI. It will no doubt be fixed. In the meantime, biframmes==0.26.0 was installed.\n",
        "It is recommended to run this notebook cell by cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VRWWL_vpNZpg"
      },
      "outputs": [],
      "source": [
        "import textwrap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoDSC70YVXog"
      },
      "outputs": [],
      "source": [
        "!pip install google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZwmwszyXSr9l"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth as google_auth\n",
        "google_auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "March 28,2024 Update\n",
        "\n",
        "There's a recent major release in the [BigFrames library](https://pypi.org/project/bigframes/#history) which is now 1.0.1, which might conflict with the Vertex AI SDK used here.\n",
        "\n",
        "BigQuery DataFrames provides a Pythonic DataFrame and machine learning (ML) API powered by the BigQuery engine.\n",
        "\n",
        "Let's install a <1.0.0\n",
        ""
      ],
      "metadata": {
        "id": "YooDjpX3VIre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bigframes==0.26.0"
      ],
      "metadata": {
        "id": "FU-rz71yT_lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "restart session before continuing"
      ],
      "metadata": {
        "id": "dnH8ZYT3WJ7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai"
      ],
      "metadata": {
        "id": "-36qtyScUiy6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnCHBV8LJhmq"
      },
      "source": [
        "# Question Answering(QA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_6EDI6AJjfT",
        "outputId": "8a79b431-d0c9-44c4-b4c6-690ccab7bf03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No, Gemini does not repeat existing\n",
            "text. It is a multimodal LLM, which\n",
            "means it can understand and generate\n",
            "text, code, images, and more. It excels\n",
            "at reasoning across vast amounts of\n",
            "information, solving complex problems,\n",
            "and adapting to various tasks through\n",
            "fine-tuning.\n"
          ]
        }
      ],
      "source": [
        "from vertexai.preview.language_models import ChatModel, InputOutputTextPair\n",
        "\n",
        "vertexai.init(project=\"aiex-57523\", location=\"us-central1\")\n",
        "chat_model = ChatModel.from_pretrained(\"chat-bison@001\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 256,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "chat = chat_model.start_chat(\n",
        "    context=\"\"\"Answer a question on the text submitted\"\"\",\n",
        ")\n",
        "from vertexai.preview.language_models import ChatModel, InputOutputTextPair\n",
        "\n",
        "vertexai.init(project=\"aiex-57523\", location=\"us-central1\")\n",
        "chat_model = ChatModel.from_pretrained(\"chat-bison@001\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 256,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "chat = chat_model.start_chat(\n",
        "    context=\"\"\"Answer a question on the text submitted\"\"\",\n",
        ")\n",
        "response = chat.send_message(\"\"\"Based on the following text, Does Gemini repeat existing text? “\n",
        "Gemini is a multimodal LLM, understanding and generating text, code, images, and more. It excels at reasoning across vast amounts of information, solving complex problems, and adapting to various tasks through fine-tuning.”\"\"\", **parameters)\n",
        "\n",
        "wrapped_text=textwrap.fill(response.text, width=40)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "from vertexai.preview.generative_models import GenerativeModel, Part\n",
        "import vertexai.preview.generative_models as generative_models\n",
        "\n",
        "import vertexai\n",
        "from vertexai.preview.generative_models import GenerativeModel, Part\n",
        "import vertexai.preview.generative_models as generative_models\n",
        "\n",
        "def generate():\n",
        "  vertexai.init(project=\"aiex-57523\", location=\"us-central1\")\n",
        "  model = GenerativeModel(\"gemini-1.0-pro-001\")\n",
        "  responses = model.generate_content(\n",
        "    \"\"\"Based on the following text, does Gemini just repeat text: Gemini: Google\\'s Multimodal AI Powerhouse\n",
        "Gemini is Google\\'s most advanced and versatile AI model yet, representing a significant leap forward in artificial intelligence capabilities. Here\\'s a breakdown of its key aspects:\n",
        "Capabilities:\n",
        "Multimodal: Unlike most AI models that specialize in one type of data (text, image, code, etc.), Gemini is natively multimodal. This means it can understand, process, and combine different information formats seamlessly. This allows it to reason and solve problems in ways that were previously impossible for AI.\n",
        "State-of-the-art performance: Across various domains, Gemini demonstrates cutting-edge capabilities in tasks like question answering, code generation, image description, and more.\n",
        "Long-context understanding: Gemini 1.5, the latest iteration, boasts a breakthrough feature: understanding and reasoning with exceptionally long stretches of information. This enables it to grasp complex contexts and nuances in a way that surpasses previous models.\n",
        "Applications:\n",
        "Scientific discovery: By analyzing vast amounts of scientific data from various sources, Gemini can assist researchers in uncovering new insights and making groundbreaking discoveries.\n",
        "Product development: Its ability to understand and generate different creative formats makes Gemini valuable for product design, marketing, and content creation.\n",
        "Personalized experiences: By tailoring its responses to individual user preferences and contexts, Gemini can power personalized experiences in education, healthcare, and other domains.\n",
        "Technical underpinnings:\n",
        "Transformer architecture: At its core, Gemini leverages the power of transformer-based neural networks. These models excel at identifying complex relationships within data, allowing Gemini to understand and process information in a nuanced way.\n",
        "Multimodal pre-training: Unlike traditional models trained on single data types, Gemini undergoes multimodal pre-training. This means it\\'s exposed to various formats from the very beginning, enabling it to develop a comprehensive understanding of the world from the outset.\n",
        "Fine-tuning: Further refinement is achieved through fine-tuning on specific tasks and datasets. This allows Gemini to specialize in particular domains and excel at specific applications.\n",
        "Overall, Gemini represents a significant step forward in AI, paving the way for a future where machines can interact with the world in a more comprehensive and human-like manner.\n",
        "It\\'s important to note that I am also part of the Gemini family of AI models, specifically the 1.0 Pro version. While I share the core capabilities of understanding and generating text, I am still under development and learning new skills every day.\"\"\",\n",
        "    generation_config={\n",
        "        \"max_output_tokens\": 2048,\n",
        "        \"temperature\": 0.9,\n",
        "        \"top_p\": 1\n",
        "    },\n",
        "    safety_settings={\n",
        "          generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "          generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "          generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "          generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "    },\n",
        "    stream=True,\n",
        "  )\n",
        "\n",
        "  for response in responses:\n",
        "    print(response.text, end=\"\")\n",
        "\n",
        "\n",
        "generate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTWGk9On59jm",
        "outputId": "75b0d38d-7d75-4cbf-9a27-9b9fb0a55dba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No, the provided text does not indicate that Gemini simply repeats text. It emphasizes that Gemini is a multimodal AI model with advanced capabilities, including understanding and processing different information formats seamlessly, state-of-the-art performance in tasks like question answering and code generation, and long-context understanding. reason and solve problems in ways that were previously impossible for AI. Gemini demonstrates cutting-edge capabilities in tasks like question answering, code generation, image description, and more. It can also understand and reason with exceptionally long stretches of information, enabling it to grasp complex contexts and nuances in a way that surpasses previous models. Gemini has a wide range of applications, including scientific discovery, product development, and personalized experiences. It is built on the powerful transformer architecture and undergoes multimodal pre-training, enabling it to develop a comprehensive understanding of the world from the outset."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M9lPXAJShrC"
      },
      "source": [
        "# Question Answering(QA) - general"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvfFMN5ySyd3",
        "outputId": "56ef9498-b413-4366-b57b-8e4fc066ba7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am powered by PaLM 2, which stands for\n",
            "Pathways Language Model 2, a large\n",
            "language model from Google AI.\n"
          ]
        }
      ],
      "source": [
        "from vertexai.preview.language_models import ChatModel, InputOutputTextPair\n",
        "\n",
        "def predict_large_language_model_sample(\n",
        "    project_id: str,\n",
        "    model_name: str,\n",
        "    temperature: float,\n",
        "    max_output_tokens: int,\n",
        "    top_p: float,\n",
        "    top_k: int,\n",
        "    location: str = \"us-central1\",\n",
        "    ) :\n",
        "    \"\"\"Predict using a Large Language Model.\"\"\"\n",
        "    vertexai.init(project=project_id, location=location)\n",
        "\n",
        "    chat_model = ChatModel.from_pretrained(model_name)\n",
        "    parameters = {\n",
        "      \"temperature\": temperature,\n",
        "      \"max_output_tokens\": max_output_tokens,\n",
        "      \"top_p\": top_p,\n",
        "      \"top_k\": top_k,\n",
        "    }\n",
        "\n",
        "    chat = chat_model.start_chat(\n",
        "      examples=[]\n",
        "    )\n",
        "    response=chat.send_message('''What Transformer model are you using for this conversation?''',**parameters)\n",
        "    #print(response.text)\n",
        "    wrapped_text=textwrap.fill(response.text, width=40)\n",
        "    print(wrapped_text)\n",
        "\n",
        "\n",
        "predict_large_language_model_sample(\"aiex-57523\", \"chat-bison@001\", 0.2, 256, 0.8, 40, \"us-central1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7Iggtu8SrI2"
      },
      "source": [
        "# Summarization of a conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJoN4TsxnjK6",
        "outputId": "e5078775-7d3d-409b-c824-bce832d4d1e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The customer wants to change the\n",
            "shipping address for an order. The\n",
            "service rep informs the customer that\n",
            "the order has already shipped and they\n",
            "will need to contact the shipping\n",
            "provider to update their delivery\n",
            "details.\n"
          ]
        }
      ],
      "source": [
        "import vertexai\n",
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "vertexai.init(project=\"aiex-57523\", location=\"us-central1\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 256,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "response = model.predict(\n",
        "    \"\"\"Summarize the following conversation between a service rep and a customer in a few sentences. Use only the information from the conversation.\n",
        "\n",
        "Service Rep: How may I assist you today?\n",
        "Customer: I need to change the shipping address for an order.\n",
        "Service Rep: Ok, I can help you with that if the order has not been fulfilled from our warehouse yet. But if it has already shipped, then you will need to contact the shipping provider. Do you have the order ID?\n",
        "Customer: Yes, it\\'s 88986367.\n",
        "Service Rep: One minute please while I pull up your order information.\n",
        "Customer: No problem\n",
        "Service Rep: Ok, it looks like your order was shipped from our warehouse 2 days ago. It is now in the hands of  the shipping provider, so you will need to contact them to update your delivery details. You can track your order with the shipping provider here: https://www.shippingprovider.com\n",
        "Customer: Sigh, ok.\n",
        "Service Rep: Is there anything else I can help you with today?\n",
        "Customer: No, thanks.\"\"\",\n",
        "    **parameters\n",
        ")\n",
        "#print(f\"Response from Model: {response.text}\")\n",
        "wrapped_text=textwrap.fill(response.text, width=40)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyaLZEkoS4W_"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQgm5sNDnxOD",
        "outputId": "269fa4bc-ed11-45c9-bcf7-dbf3ea4ad112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ],
      "source": [
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "vertexai.init(project=\"aiex-57523\", location=\"us-central1\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 256,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "response = model.predict(\n",
        "    \"\"\"Is the sentiment positive or negative towards Louis van Gaal based on the article:\n",
        "\n",
        "Article:\n",
        "Louis van Gaal said he had no option but to substitute Paddy McNair in the first half against Southampton because the defender\\'s \\'confidence\\' was shot - but believes that it will benefit the youngster in the long run.\n",
        "The 19-year-old was hooked by Van Gaal after only 39 minutes at St Mary\\'s Stadium on Monday night during Manchester United\\'s 2-1 victory over the Saints.\n",
        "McNair was struggling to contain Southampton strikers Shane Long and Graziano Pelle, forcing Van Gaal into replacing him prematurely.\n",
        "Speaking to Sky Sports after the match, Van Gaal explained: \\'He (McNair) hadn\\'t any confidence. He had already given three big chances away.\n",
        "\\'I had to (substitute him), it\\'s very disappointing for me and also for Paddy, but I had to because as a manager, I\\'m responsible to win.\n",
        "\\'And I think, after the change, we played a little better.\\'\n",
        "But in spite of the fact United won the game, McNair was exposed time after time in defence and was substituted - even though Chris Smalling had already departed early with an injury.\n",
        "Jonny Evans came on to replace Smalling, before McNair made way for midfielder Ander Herrera as Michael Carrick dropped back in to the centre of defence in Van Gaal\\'s 3-5-2 system.\n",
        "And, despite admitting it will be difficult for McNair to accept being replaced so early, Van Gaal insisted that it was a necessity which will serve the Northern Irishman well long term.\n",
        "Van Gaal continued: \\'Of course, it\\'s tough (for McNair), but it\\'s also in his best interests.\\'\n",
        "The victory moved United up to third in the Premier League - their highest position since they claimed the title in 2012-13 under Sir Alex Ferguson. \"\"\",\n",
        "    **parameters\n",
        ")\n",
        "#print(f\"Response from Model: {response.text}\")\n",
        "wrapped_text=textwrap.fill(response.text, width=40)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IeUFdFGWSAp",
        "outputId": "d9df8bce-a965-451c-9688-80313b27a850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentiment towards Louis van Gaal is\n",
            "positive.  The article is about Louis\n",
            "van Gaal's decision to substitute Paddy\n",
            "McNair in the first half against\n",
            "Southampton. The article says that Van\n",
            "Gaal had to make the substitution\n",
            "because McNair was struggling and that\n",
            "it was a necessary decision. The article\n",
            "also says that Van Gaal believes that\n",
            "the substitution will benefit McNair in\n",
            "the long run.  Overall, the article\n",
            "presents Van Gaal in a positive light.\n",
            "He is portrayed as a manager who is\n",
            "making tough decisions in order to win\n",
            "games and who is looking out for the\n",
            "best interests of his players.\n"
          ]
        }
      ],
      "source": [
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "vertexai.init(project=\"aiex-57523\", location=\"us-central1\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 256,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "response = model.predict(\n",
        "    \"\"\"Is the sentiment positive or negative towards Louis van Gaal based on the article and explain why in detail:\n",
        "\n",
        "Article:\n",
        "Louis van Gaal said he had no option but to substitute Paddy McNair in the first half against Southampton because the defender\\'s \\'confidence\\' was shot - but believes that it will benefit the youngster in the long run.\n",
        "The 19-year-old was hooked by Van Gaal after only 39 minutes at St Mary\\'s Stadium on Monday night during Manchester United\\'s 2-1 victory over the Saints.\n",
        "McNair was struggling to contain Southampton strikers Shane Long and Graziano Pelle, forcing Van Gaal into replacing him prematurely.\n",
        "Speaking to Sky Sports after the match, Van Gaal explained: \\'He (McNair) hadn\\'t any confidence. He had already given three big chances away.\n",
        "\\'I had to (substitute him), it\\'s very disappointing for me and also for Paddy, but I had to because as a manager, I\\'m responsible to win.\n",
        "\\'And I think, after the change, we played a little better.\\'\n",
        "But in spite of the fact United won the game, McNair was exposed time after time in defence and was substituted - even though Chris Smalling had already departed early with an injury.\n",
        "Jonny Evans came on to replace Smalling, before McNair made way for midfielder Ander Herrera as Michael Carrick dropped back in to the centre of defence in Van Gaal\\'s 3-5-2 system.\n",
        "And, despite admitting it will be difficult for McNair to accept being replaced so early, Van Gaal insisted that it was a necessity which will serve the Northern Irishman well long term.\n",
        "Van Gaal continued: \\'Of course, it\\'s tough (for McNair), but it\\'s also in his best interests.\\'\n",
        "The victory moved United up to third in the Premier League - their highest position since they claimed the title in 2012-13 under Sir Alex Ferguson. \"\"\",\n",
        "    **parameters\n",
        ")\n",
        "#print(f\"Response from Model: {response.text}\")\n",
        "wrapped_text=textwrap.fill(response.text, width=40)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rilcm2bGS-sC"
      },
      "source": [
        "# Multi-choice problems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfMQS2dqoBoF",
        "outputId": "a792aac3-80d6-450e-ca28-5799c8347c8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "culture\n"
          ]
        }
      ],
      "source": [
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "vertexai.init(project=\"aiex-57523\", location=\"us-central1\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 256,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "response = model.predict(\n",
        "    \"\"\"Multi-choice problem: What is the topic of this text?\n",
        "- entertainment\n",
        "- technology\n",
        "- politics\n",
        "- sports\n",
        "- business\n",
        "- health\n",
        "- fun\n",
        "- culture\n",
        "- science\n",
        "\n",
        "Text: Samba, is a name or prefix used for several rhythmic variants, such as samba urbano carioca (urban Carioca samba), samba de roda (sometimes also called rural samba), recognized as part of the Intangible Cultural Heritage of Humanity by UNESCO, amongst many other forms of Samba, mostly originated in the Rio de Janeiro and Bahia States. Samba is a broad term for many of the rhythms that compose the better known Brazilian music genres that originated in the Afro-Brazilian communities of Bahia in the late 19th century and early 20th century, having continued its development on the communities of Rio de Janeiro in the early 20th century. Having its roots in the Afro-Brazilian Candomblé, as well as other Afro-Brazilian and Indigenous folk traditions, such as the traditional Samba de Caboclo, it is considered one of the most important cultural phenomena in Brazil and one of the country\\'s symbols. Present in the Portuguese language at least since the 19th century, the word \\\"samba\\\" was originally used to designate a \\\"popular dance\\\". Over time, its meaning has been extended to a \\\"batuque-like circle dance\\\", a dance style, and also to a \\\"music genre\\\". This process of establishing itself as a musical genre began in the 1910s and it had its inaugural landmark in the song \\\"Pelo Telefone\\\", launched in 1917. Despite being identified by its creators, the public, and the Brazilian music industry as \\\"samba\\\", this pioneering style was much more connected from the rhythmic and instrumental point of view to maxixe than to samba itself.\n",
        "\n",
        "Samba was modernly structured as a musical genre only in the late 1920s from the neighborhood of Estácio and soon extended to Oswaldo Cruz and other parts of Rio through its commuter rail. Today synonymous with the rhythm of samba, this new samba brought innovations in rhythm, melody and also in thematic aspects. Its rhythmic change based on a new percussive instrumental pattern resulted in a more \\\"batucado\\\" and syncopated style – as opposed to the inaugural \\\"samba-maxixe\\\" – notably characterized by a faster tempo, longer notes and a characterized cadence far beyond the simple ones palms used so far. Also the \\\"Estácio paradigm\\\" innovated in the formatting of samba as a song, with its musical organization in first and second parts in both melody and lyrics. In this way, the sambistas of Estácio created, structured and redefined the urban Carioca samba as a genre in a modern and finished way. In this process of establishment as an urban and modern musical expression, the Carioca samba had the decisive role of samba schools, responsible for defining and legitimizing definitively the aesthetic bases of rhythm, and radio broadcasting, which greatly contributed to the diffusion and popularization of the genre and its song singers. Thus, samba has achieved major projection throughout Brazil and has become one of the main symbols of Brazilian national identity. Once criminalized and rejected for its Afro-Brazilian origins, and definitely working-class music in its mythic origins, the genre has also received support from members of the upper classes and the country\\'s cultural elite.\"\"\",\n",
        "    **parameters\n",
        ")\n",
        "#print(f\"Response from Model: {response.text}\")\n",
        "wrapped_text=textwrap.fill(response.text, width=40)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsxGgCS5XjBv"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rw2TkOsGXkiV",
        "outputId": "5e869e51-9b1c-417b-b6ec-6b67406594ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Model: Sure, here is the code for a function in Python that calculates a Fibonacci suite up to 10:\n",
            "\n",
            "```python\n",
            "def fibonacci(n):\n",
            "  \"\"\"This function calculates the Fibonacci suite up to n.\n",
            "\n",
            "  Args:\n",
            "    n: The number of terms in the Fibonacci suite to calculate.\n",
            "\n",
            "  Returns:\n",
            "    A list of the Fibonacci numbers up to n.\n",
            "  \"\"\"\n",
            "\n",
            "  # Initialize the Fibonacci sequence.\n",
            "  a, b = 0, 1\n",
            "\n",
            "  # Create a list to store the Fibonacci numbers.\n",
            "  fibonacci_numbers = []\n",
            "\n",
            "  # Add the first two Fibonacci numbers to the list.\n",
            "  fibonacci_numbers.append(a)\n",
            "  fibonacci_numbers.append(b)\n",
            "\n",
            "  # Iterate over the numbers from 2 to n, and calculate the next Fibonacci number.\n",
            "  for i in range(2, n):\n",
            "    c = a + b\n",
            "    a = b\n",
            "    b = c\n",
            "    fibonacci_numbers.append(c)\n",
            "\n",
            "  # Return the list of Fibonacci numbers.\n",
            "  return fibonacci_numbers\n",
            "\n",
            "```\n",
            "\n",
            "Here is an example of how to use this function:\n",
            "\n",
            "```python\n",
            ">>> fibonacci(10)\n",
            "[0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "from vertexai.preview.language_models import CodeChatModel\n",
        "\n",
        "vertexai.init(project=\"aiex-57523\", location=\"us-central1\")\n",
        "chat_model = CodeChatModel.from_pretrained(\"codechat-bison@001\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.5,\n",
        "    \"max_output_tokens\": 1024\n",
        "}\n",
        "chat = chat_model.start_chat()\n",
        "response = chat.send_message(\"\"\"Write the code in Python for a function in Python that calculates a Fibonacci  suite up to 10:\"\"\", **parameters)\n",
        "print(f\"Response from Model: {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmExDk96XqqJ"
      },
      "source": [
        "copy code into cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SHM1bskUXtGc"
      },
      "outputs": [],
      "source": [
        "def fibonacci(n):\n",
        "  \"\"\"\n",
        "  This function calculates the Fibonacci sequence up to the given number.\n",
        "\n",
        "  Args:\n",
        "    n: The number to calculate the Fibonacci sequence up to.\n",
        "\n",
        "  Returns:\n",
        "    The Fibonacci sequence up to the given number.\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize the sequence with 0 and 1.\n",
        "  a = 0\n",
        "  b = 1\n",
        "\n",
        "  # Iterate through the sequence, calculating each new number.\n",
        "  for i in range(n):\n",
        "    # Calculate the next number in the sequence.\n",
        "    c = a + b\n",
        "\n",
        "    # Update the sequence.\n",
        "    a = b\n",
        "    b = c\n",
        "\n",
        "  # Return the sequence.\n",
        "  return a, b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZwHyuSHXyZr"
      },
      "source": [
        "call the function and run the code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIZLkZvwXyAA",
        "outputId": "e56c5a54-f487-47fc-f266-9d5fbe58c037"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(55, 89)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "fibonacci(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsuEsPSsDd0k",
        "outputId": "4ebd2976-c3e1-40e6-a877-44624fedf6fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Model: ```python\n",
            "from vertexai.preview.language_models import CodeChatModel\n",
            "\n",
            "vertexai.init(project=\"aiex-57523\", location=\"us-central1\")\n",
            "chat_model = CodeChatModel.from_pretrained(\"codechat-bison@001\")\n",
            "parameters = {\n",
            "    \"temperature\": 0.5,\n",
            "    \"max_output_tokens\": 1024\n",
            "}\n",
            "chat = chat_model.start_chat()\n",
            "response = chat.send_message(\"\"\"Write the code in Python for a function in Python that calculates a Fibonacci  suite up to 10:\"\"\", **parameters)\n",
            "print(f\"Response from Model: {response.text}\")\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "import vertexai\n",
        "from vertexai.preview.language_models import CodeGenerationModel\n",
        "\n",
        "vertexai.init(project=\"aiex-57523\", location=\"us-central1\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 1024\n",
        "}\n",
        "model = CodeGenerationModel.from_pretrained(\"code-bison@001\")\n",
        "response = model.predict(\n",
        "    prefix = \"\"\"Correct the following code in Python that calculates a Fibonacci suite up to 10 numbers starting from 0 or 1. The code is inaccurate because it calculates 11 numbers instead of 10. Provide a corrected code:from vertexai.preview.language_models import CodeChatModel\n",
        "\n",
        "vertexai.init(project=\\\"aiex-57523\\\", location=\\\"us-central1\\\")\n",
        "chat_model = CodeChatModel.from_pretrained(\\\"codechat-bison@001\\\")\n",
        "parameters = {\n",
        "    \\\"temperature\\\": 0.5,\n",
        "    \\\"max_output_tokens\\\": 1024\n",
        "}\n",
        "chat = chat_model.start_chat()\n",
        "response = chat.send_message(\\\"\\\"\\\"Write the code in Python for a function in Python that calculates a Fibonacci  suite up to 10:\\\"\\\"\\\", **parameters)\n",
        "print(f\\\"Response from Model: {response.text}\\\")\"\"\",\n",
        "    **parameters\n",
        ")\n",
        "print(f\"Response from Model: {response.text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSmbU3ztFk9v",
        "outputId": "bfd5e66f-e74b-45d6-bb75-d29cf432f811"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55\n"
          ]
        }
      ],
      "source": [
        "def fibonacci(n):\n",
        "  \"\"\"Calculates the nth Fibonacci number.\n",
        "\n",
        "  Args:\n",
        "    n: The nth Fibonacci number to calculate.\n",
        "\n",
        "  Returns:\n",
        "    The nth Fibonacci number.\n",
        "  \"\"\"\n",
        "  if n == 0:\n",
        "    return 0\n",
        "  elif n == 1:\n",
        "    return 1\n",
        "  else:\n",
        "    return fibonacci(n - 1) + fibonacci(n - 2)\n",
        "\n",
        "\n",
        "print(fibonacci(10))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}