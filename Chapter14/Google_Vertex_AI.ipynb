{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPl5j22LDm50w9prxR0/2k1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Vertex AI\n",
        "Copyright 2023, Denis Rothman\n",
        "\n",
        "Google as other editors advancing at full speed with LLM generative transformer models are continually updating the available models.\n",
        "\n",
        "The models versions may not be stable. If you encounter an issue, go to Google's model versioning page to see which use.\n",
        "\n",
        "https://cloud.google.com/vertex-ai/docs/generative-ai/learn/model-versioning\n"
      ],
      "metadata": {
        "id": "PYvzs7cLR9oY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap"
      ],
      "metadata": {
        "id": "VRWWL_vpNZpg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Answer \"Y\" when prompted to uninstall Shapely**"
      ],
      "metadata": {
        "id": "IETFPwM4J2HZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall shapely -qq"
      ],
      "metadata": {
        "id": "Oo4iyMMNTq4m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73e398f0-2788-45a5-f9bd-4418e0fd66e4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proceed (Y/n)? Y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shapely==1.8.1 -qq"
      ],
      "metadata": {
        "id": "W5eT_j0ST76K"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-aiplatform"
      ],
      "metadata": {
        "id": "MoDSC70YVXog",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcddd6df-3b2f-4b8c-8109-0c24ad2699b6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-aiplatform in /usr/local/lib/python3.10/dist-packages (1.27.0)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.11.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.22.3)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.20.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (23.1)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.10.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.10.1)\n",
            "Requirement already satisfied: shapely<2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.8.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.59.1)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.17.3)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.27.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.56.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.48.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.2)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.12.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.4)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Restart the runtime and go to \"Runtime\" menu and select \"Run all\"**"
      ],
      "metadata": {
        "id": "Mrt3KCsrJ7JT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth as google_auth\n",
        "google_auth.authenticate_user()\n",
        "\n",
        "import vertexai"
      ],
      "metadata": {
        "id": "ZwmwszyXSr9l"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question Answering(QA)"
      ],
      "metadata": {
        "id": "lnCHBV8LJhmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.preview.language_models import ChatModel, InputOutputTextPair\n",
        "\n",
        "vertexai.init(project=\"aiex-57523\", location=\"us-central1\")\n",
        "chat_model = ChatModel.from_pretrained(\"chat-bison@001\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 256,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "chat = chat_model.start_chat(\n",
        "    context=\"\"\"Answer a question on the text submitted\"\"\",\n",
        ")\n",
        "response = chat.send_message(\"\"\"Based on the following text, Does BARD repeat existing text? “Bard, like some other standalone LLM experiences, is intended to generate original content and not replicate existing content at length. We’ve designed our systems to limit the chances of this occurring, and we will continue to improve how these systems function. If Bard does directly quote at length from a webpage, it cites that page. For answers with image thumbnails, Bard enables users to easily see and click to navigate directly to a source for each image.\n",
        "\n",
        "Sometimes the same content may be found on multiple webpages and Bard attempts to point to a popular source. In the case of citations to code repositories, the citation may also reference an applicable open source license.\n",
        "\n",
        "Bard was built to be a creative and helpful collaborator—it works well in creative tasks like helping you write an email or brainstorm ideas for a birthday party. We see it as a complementary experience to Google Search. That’s why we added the “Google It” button to Bard, so people can easily move from Bard to explore information from across the web.\n",
        "\n",
        "Bard is an experiment, and we\\'ll use its launch as an opportunity to learn, iterate, and improve the experience as we get feedback from a range of stakeholders including people like you, publishers, creators, and more.”\"\"\", **parameters)\n",
        "\n",
        "wrapped_text=textwrap.fill(response.text, width=40)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_6EDI6AJjfT",
        "outputId": "2ece2727-3a2b-4104-c0fc-60905a696de8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No, Bard is not intended to repeat\n",
            "existing text. It is designed to\n",
            "generate original content and not\n",
            "replicate existing content at length.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question Answering(QA) - general"
      ],
      "metadata": {
        "id": "5M9lPXAJShrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.preview.language_models import ChatModel, InputOutputTextPair\n",
        "\n",
        "def predict_large_language_model_sample(\n",
        "    project_id: str,\n",
        "    model_name: str,\n",
        "    temperature: float,\n",
        "    max_output_tokens: int,\n",
        "    top_p: float,\n",
        "    top_k: int,\n",
        "    location: str = \"us-central1\",\n",
        "    ) :\n",
        "    \"\"\"Predict using a Large Language Model.\"\"\"\n",
        "    vertexai.init(project=project_id, location=location)\n",
        "\n",
        "    chat_model = ChatModel.from_pretrained(model_name)\n",
        "    parameters = {\n",
        "      \"temperature\": temperature,\n",
        "      \"max_output_tokens\": max_output_tokens,\n",
        "      \"top_p\": top_p,\n",
        "      \"top_k\": top_k,\n",
        "    }\n",
        "\n",
        "    chat = chat_model.start_chat(\n",
        "      examples=[]\n",
        "    )\n",
        "    response=chat.send_message('''What Transformer model are you using for this conversation?''',**parameters)\n",
        "    #print(response.text)\n",
        "    wrapped_text=textwrap.fill(response.text, width=40)\n",
        "    print(wrapped_text)\n",
        "\n",
        "\n",
        "predict_large_language_model_sample(\"aiex-57523\", \"chat-bison@001\", 0.2, 256, 0.8, 40, \"us-central1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvfFMN5ySyd3",
        "outputId": "e8a07b92-b7be-4667-d8ce-37d1fb717fea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am powered by PaLM 2, which stands for\n",
            "Pathways Language Model 2. PaLM 2 was\n",
            "trained by a team of engineers and\n",
            "scientists at Google AI.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarization of a conversation"
      ],
      "metadata": {
        "id": "u7Iggtu8SrI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "vertexai.init(project=\"aiex-57523\", location=\"us-central1\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 256,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "response = model.predict(\n",
        "    \"\"\"Summarize the following conversation between a service rep and a customer in a few sentences. Use only the information from the conversation.\n",
        "\n",
        "Service Rep: How may I assist you today?\n",
        "Customer: I need to change the shipping address for an order.\n",
        "Service Rep: Ok, I can help you with that if the order has not been fulfilled from our warehouse yet. But if it has already shipped, then you will need to contact the shipping provider. Do you have the order ID?\n",
        "Customer: Yes, it\\'s 88986367.\n",
        "Service Rep: One minute please while I pull up your order information.\n",
        "Customer: No problem\n",
        "Service Rep: Ok, it looks like your order was shipped from our warehouse 2 days ago. It is now in the hands of  the shipping provider, so you will need to contact them to update your delivery details. You can track your order with the shipping provider here: https://www.shippingprovider.com\n",
        "Customer: Sigh, ok.\n",
        "Service Rep: Is there anything else I can help you with today?\n",
        "Customer: No, thanks.\"\"\",\n",
        "    **parameters\n",
        ")\n",
        "#print(f\"Response from Model: {response.text}\")\n",
        "wrapped_text=textwrap.fill(response.text, width=40)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJoN4TsxnjK6",
        "outputId": "bfcd8347-bdd8-4782-b21d-b2b36625ac3c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The customer wants to change the\n",
            "shipping address for an order. The\n",
            "service rep informs the customer that\n",
            "the order has already shipped and they\n",
            "will need to contact the shipping\n",
            "provider to update their delivery\n",
            "details. The customer is not happy but\n",
            "agrees to contact the shipping provider.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis"
      ],
      "metadata": {
        "id": "qyaLZEkoS4W_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "vertexai.init(project=\"aiex-57523\", location=\"us-central1\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 256,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "response = model.predict(\n",
        "    \"\"\"Is the sentiment positive or negative towards Louis van Gaal based on the article:\n",
        "\n",
        "Article:\n",
        "Louis van Gaal said he had no option but to substitute Paddy McNair in the first half against Southampton because the defender\\'s \\'confidence\\' was shot - but believes that it will benefit the youngster in the long run.\n",
        "The 19-year-old was hooked by Van Gaal after only 39 minutes at St Mary\\'s Stadium on Monday night during Manchester United\\'s 2-1 victory over the Saints.\n",
        "McNair was struggling to contain Southampton strikers Shane Long and Graziano Pelle, forcing Van Gaal into replacing him prematurely.\n",
        "Speaking to Sky Sports after the match, Van Gaal explained: \\'He (McNair) hadn\\'t any confidence. He had already given three big chances away.\n",
        "\\'I had to (substitute him), it\\'s very disappointing for me and also for Paddy, but I had to because as a manager, I\\'m responsible to win.\n",
        "\\'And I think, after the change, we played a little better.\\'\n",
        "But in spite of the fact United won the game, McNair was exposed time after time in defence and was substituted - even though Chris Smalling had already departed early with an injury.\n",
        "Jonny Evans came on to replace Smalling, before McNair made way for midfielder Ander Herrera as Michael Carrick dropped back in to the centre of defence in Van Gaal\\'s 3-5-2 system.\n",
        "And, despite admitting it will be difficult for McNair to accept being replaced so early, Van Gaal insisted that it was a necessity which will serve the Northern Irishman well long term.\n",
        "Van Gaal continued: \\'Of course, it\\'s tough (for McNair), but it\\'s also in his best interests.\\'\n",
        "The victory moved United up to third in the Premier League - their highest position since they claimed the title in 2012-13 under Sir Alex Ferguson. \"\"\",\n",
        "    **parameters\n",
        ")\n",
        "#print(f\"Response from Model: {response.text}\")\n",
        "wrapped_text=textwrap.fill(response.text, width=40)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQgm5sNDnxOD",
        "outputId": "cc8f8abd-85b0-4dda-9c79-3e943b4196a9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive.  The article is about Louis\n",
            "van Gaal's decision to substitute Paddy\n",
            "McNair in the first half of the game\n",
            "against Southampton. The article states\n",
            "that Van Gaal had to make the\n",
            "substitution because McNair was\n",
            "struggling and that it was in McNair's\n",
            "best interests. The article also states\n",
            "that Van Gaal believes that the\n",
            "substitution will benefit McNair in the\n",
            "long run. This suggests that the article\n",
            "has a positive sentiment towards Van\n",
            "Gaal.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "vertexai.init(project=\"aiex-57523\", location=\"us-central1\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 256,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "response = model.predict(\n",
        "    \"\"\"Is the sentiment positive or negative towards Louis van Gaal based on the article and explain why in detail:\n",
        "\n",
        "Article:\n",
        "Louis van Gaal said he had no option but to substitute Paddy McNair in the first half against Southampton because the defender\\'s \\'confidence\\' was shot - but believes that it will benefit the youngster in the long run.\n",
        "The 19-year-old was hooked by Van Gaal after only 39 minutes at St Mary\\'s Stadium on Monday night during Manchester United\\'s 2-1 victory over the Saints.\n",
        "McNair was struggling to contain Southampton strikers Shane Long and Graziano Pelle, forcing Van Gaal into replacing him prematurely.\n",
        "Speaking to Sky Sports after the match, Van Gaal explained: \\'He (McNair) hadn\\'t any confidence. He had already given three big chances away.\n",
        "\\'I had to (substitute him), it\\'s very disappointing for me and also for Paddy, but I had to because as a manager, I\\'m responsible to win.\n",
        "\\'And I think, after the change, we played a little better.\\'\n",
        "But in spite of the fact United won the game, McNair was exposed time after time in defence and was substituted - even though Chris Smalling had already departed early with an injury.\n",
        "Jonny Evans came on to replace Smalling, before McNair made way for midfielder Ander Herrera as Michael Carrick dropped back in to the centre of defence in Van Gaal\\'s 3-5-2 system.\n",
        "And, despite admitting it will be difficult for McNair to accept being replaced so early, Van Gaal insisted that it was a necessity which will serve the Northern Irishman well long term.\n",
        "Van Gaal continued: \\'Of course, it\\'s tough (for McNair), but it\\'s also in his best interests.\\'\n",
        "The victory moved United up to third in the Premier League - their highest position since they claimed the title in 2012-13 under Sir Alex Ferguson. \"\"\",\n",
        "    **parameters\n",
        ")\n",
        "#print(f\"Response from Model: {response.text}\")\n",
        "wrapped_text=textwrap.fill(response.text, width=40)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IeUFdFGWSAp",
        "outputId": "8ff04b96-8ff2-4f1f-8da0-49553b2dee3c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentiment towards Louis van Gaal in\n",
            "the article is positive.  The article is\n",
            "about Louis van Gaal's decision to\n",
            "substitute Paddy McNair in the first\n",
            "half of the Manchester United vs.\n",
            "Southampton game. The article states\n",
            "that Van Gaal had to make the\n",
            "substitution because McNair was\n",
            "struggling and had given away three big\n",
            "chances. Van Gaal also says that he\n",
            "believes the substitution will benefit\n",
            "McNair in the long run.  The article\n",
            "also states that Van Gaal is responsible\n",
            "for winning the game, and that he\n",
            "believes the substitution helped United\n",
            "play better.  Overall, the article\n",
            "presents Van Gaal in a positive light.\n",
            "He is portrayed as a manager who is\n",
            "willing to make tough decisions in order\n",
            "to win, and who is looking out for the\n",
            "best interests of his players.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-choice problems"
      ],
      "metadata": {
        "id": "rilcm2bGS-sC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "vertexai.init(project=\"aiex-57523\", location=\"us-central1\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 256,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "response = model.predict(\n",
        "    \"\"\"Multi-choice problem: What is the topic of this text?\n",
        "- entertainment\n",
        "- technology\n",
        "- politics\n",
        "- sports\n",
        "- business\n",
        "- health\n",
        "- fun\n",
        "- culture\n",
        "- science\n",
        "\n",
        "Text: Samba, is a name or prefix used for several rhythmic variants, such as samba urbano carioca (urban Carioca samba), samba de roda (sometimes also called rural samba), recognized as part of the Intangible Cultural Heritage of Humanity by UNESCO, amongst many other forms of Samba, mostly originated in the Rio de Janeiro and Bahia States. Samba is a broad term for many of the rhythms that compose the better known Brazilian music genres that originated in the Afro-Brazilian communities of Bahia in the late 19th century and early 20th century, having continued its development on the communities of Rio de Janeiro in the early 20th century. Having its roots in the Afro-Brazilian Candomblé, as well as other Afro-Brazilian and Indigenous folk traditions, such as the traditional Samba de Caboclo, it is considered one of the most important cultural phenomena in Brazil and one of the country\\'s symbols. Present in the Portuguese language at least since the 19th century, the word \\\"samba\\\" was originally used to designate a \\\"popular dance\\\". Over time, its meaning has been extended to a \\\"batuque-like circle dance\\\", a dance style, and also to a \\\"music genre\\\". This process of establishing itself as a musical genre began in the 1910s and it had its inaugural landmark in the song \\\"Pelo Telefone\\\", launched in 1917. Despite being identified by its creators, the public, and the Brazilian music industry as \\\"samba\\\", this pioneering style was much more connected from the rhythmic and instrumental point of view to maxixe than to samba itself.\n",
        "\n",
        "Samba was modernly structured as a musical genre only in the late 1920s from the neighborhood of Estácio and soon extended to Oswaldo Cruz and other parts of Rio through its commuter rail. Today synonymous with the rhythm of samba, this new samba brought innovations in rhythm, melody and also in thematic aspects. Its rhythmic change based on a new percussive instrumental pattern resulted in a more \\\"batucado\\\" and syncopated style – as opposed to the inaugural \\\"samba-maxixe\\\" – notably characterized by a faster tempo, longer notes and a characterized cadence far beyond the simple ones palms used so far. Also the \\\"Estácio paradigm\\\" innovated in the formatting of samba as a song, with its musical organization in first and second parts in both melody and lyrics. In this way, the sambistas of Estácio created, structured and redefined the urban Carioca samba as a genre in a modern and finished way. In this process of establishment as an urban and modern musical expression, the Carioca samba had the decisive role of samba schools, responsible for defining and legitimizing definitively the aesthetic bases of rhythm, and radio broadcasting, which greatly contributed to the diffusion and popularization of the genre and its song singers. Thus, samba has achieved major projection throughout Brazil and has become one of the main symbols of Brazilian national identity. Once criminalized and rejected for its Afro-Brazilian origins, and definitely working-class music in its mythic origins, the genre has also received support from members of the upper classes and the country\\'s cultural elite.\"\"\",\n",
        "    **parameters\n",
        ")\n",
        "#print(f\"Response from Model: {response.text}\")\n",
        "wrapped_text=textwrap.fill(response.text, width=40)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfMQS2dqoBoF",
        "outputId": "8d848310-b17c-4476-8d10-da46891a91aa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The topic of this text is culture. The\n",
            "text talks about the history of samba,\n",
            "which is a Brazilian music genre. The\n",
            "text also talks about the origins of\n",
            "samba and how it has become one of the\n",
            "most important cultural phenomena in\n",
            "Brazil.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code"
      ],
      "metadata": {
        "id": "BsxGgCS5XjBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.preview.language_models import CodeChatModel\n",
        "\n",
        "vertexai.init(project=\"aiex-57523\", location=\"us-central1\")\n",
        "chat_model = CodeChatModel.from_pretrained(\"codechat-bison@001\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.5,\n",
        "    \"max_output_tokens\": 1024\n",
        "}\n",
        "chat = chat_model.start_chat()\n",
        "response = chat.send_message(\"\"\"Write the code in Python for a function in Python that calculats a Fibonacci  suite up to 10:\"\"\", **parameters)\n",
        "print(f\"Response from Model: {response.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rw2TkOsGXkiV",
        "outputId": "e72748aa-bb2c-42e3-a4c9-ac244a6f7b5f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Model: The Fibonacci sequence is a series of numbers where each number is the sum of the two preceding numbers. The sequence starts with 0 and 1, and then each number is the sum of the two preceding numbers. So, the sequence goes like this:\n",
            "\n",
            "```\n",
            "0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...\n",
            "```\n",
            "\n",
            "You can write a function in Python to calculate the Fibonacci sequence up to any number. Here is an example:\n",
            "\n",
            "```\n",
            "def fibonacci(n):\n",
            "  \"\"\"\n",
            "  This function calculates the Fibonacci sequence up to the given number.\n",
            "\n",
            "  Parameters:\n",
            "    n: The number to calculate the Fibonacci sequence up to.\n",
            "\n",
            "  Returns:\n",
            "    The Fibonacci sequence up to the given number.\n",
            "  \"\"\"\n",
            "\n",
            "  # Initialize the sequence with 0 and 1.\n",
            "  a = 0\n",
            "  b = 1\n",
            "\n",
            "  # Iterate through the sequence until we reach the given number.\n",
            "  for i in range(n):\n",
            "    # Calculate the next number in the sequence.\n",
            "    c = a + b\n",
            "\n",
            "    # Update the sequence.\n",
            "    a = b\n",
            "    b = c\n",
            "\n",
            "  # Return the sequence.\n",
            "  return [a, b]\n",
            "```\n",
            "\n",
            "This function takes a number as input and returns the Fibonacci sequence up to that number. For example, if you call the function with the number 10, it will return the following list:\n",
            "\n",
            "```\n",
            "[0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "copy code into cell"
      ],
      "metadata": {
        "id": "rmExDk96XqqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fibonacci(n):\n",
        "  \"\"\"\n",
        "  This function calculates the Fibonacci sequence up to the given number.\n",
        "\n",
        "  Args:\n",
        "    n: The number to calculate the Fibonacci sequence up to.\n",
        "\n",
        "  Returns:\n",
        "    The Fibonacci sequence up to the given number.\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize the sequence with 0 and 1.\n",
        "  a = 0\n",
        "  b = 1\n",
        "\n",
        "  # Iterate through the sequence, calculating each new number.\n",
        "  for i in range(n):\n",
        "    # Calculate the next number in the sequence.\n",
        "    c = a + b\n",
        "\n",
        "    # Update the sequence.\n",
        "    a = b\n",
        "    b = c\n",
        "\n",
        "  # Return the sequence.\n",
        "  return a, b"
      ],
      "metadata": {
        "id": "SHM1bskUXtGc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "call the function and run the code"
      ],
      "metadata": {
        "id": "PZwHyuSHXyZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fibonacci(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIZLkZvwXyAA",
        "outputId": "1d0e99de-0dfe-4ff8-f7dc-51b7d77c5ac9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(55, 89)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "from vertexai.preview.language_models import CodeGenerationModel\n",
        "\n",
        "vertexai.init(project=\"aiex-57523\", location=\"us-central1\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 1024\n",
        "}\n",
        "model = CodeGenerationModel.from_pretrained(\"code-bison@001\")\n",
        "response = model.predict(\n",
        "    prefix = \"\"\"Correct the following code in Python that calculates a Fibonacci suite up to 10 numbers starting from 0 or 1. The code is inaccurate because it calculates 11 numbers instead of 10. Provide a corrected code:from vertexai.preview.language_models import CodeChatModel\n",
        "\n",
        "vertexai.init(project=\\\"aiex-57523\\\", location=\\\"us-central1\\\")\n",
        "chat_model = CodeChatModel.from_pretrained(\\\"codechat-bison@001\\\")\n",
        "parameters = {\n",
        "    \\\"temperature\\\": 0.5,\n",
        "    \\\"max_output_tokens\\\": 1024\n",
        "}\n",
        "chat = chat_model.start_chat()\n",
        "response = chat.send_message(\\\"\\\"\\\"Write the code in Python for a function in Python that calculats a Fibonacci  suite up to 10:\\\"\\\"\\\", **parameters)\n",
        "print(f\\\"Response from Model: {response.text}\\\")\"\"\",\n",
        "    **parameters\n",
        ")\n",
        "print(f\"Response from Model: {response.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsuEsPSsDd0k",
        "outputId": "15a06c63-b9dc-4da4-f648-b96751d04a8d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Model: ```python\n",
            "def fibonacci(n):\n",
            "  \"\"\"Calculates the nth Fibonacci number.\n",
            "\n",
            "  Args:\n",
            "    n: The nth Fibonacci number to calculate.\n",
            "\n",
            "  Returns:\n",
            "    The nth Fibonacci number.\n",
            "  \"\"\"\n",
            "  if n == 0:\n",
            "    return 0\n",
            "  elif n == 1:\n",
            "    return 1\n",
            "  else:\n",
            "    return fibonacci(n - 1) + fibonacci(n - 2)\n",
            "\n",
            "\n",
            "print(fibonacci(10))\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fibonacci(n):\n",
        "  \"\"\"Calculates the nth Fibonacci number.\n",
        "\n",
        "  Args:\n",
        "    n: The nth Fibonacci number to calculate.\n",
        "\n",
        "  Returns:\n",
        "    The nth Fibonacci number.\n",
        "  \"\"\"\n",
        "  if n == 0:\n",
        "    return 0\n",
        "  elif n == 1:\n",
        "    return 1\n",
        "  else:\n",
        "    return fibonacci(n - 1) + fibonacci(n - 2)\n",
        "\n",
        "\n",
        "print(fibonacci(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSmbU3ztFk9v",
        "outputId": "194191a2-8707-471f-b3f8-1a5cc9a92f02"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55\n"
          ]
        }
      ]
    }
  ]
}
