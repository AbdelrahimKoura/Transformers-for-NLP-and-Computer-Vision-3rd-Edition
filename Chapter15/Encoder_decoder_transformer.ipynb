{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8CMRQEUz5v4"
      },
      "source": [
        "# Encoder-decoder Transformer\n",
        "\n",
        "Copyright 2023, Denis Rothman\n",
        "\n",
        "Generated by OpenAI GPT-4 through advanced prompt engineering\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2re7iEKZpRQ"
      },
      "source": [
        "#Library installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ErcnTN3CZLmo",
        "outputId": "6d9e83a7-5b05-432b-be61-920782814609",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.11.17)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4 requests nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code is a command-line instruction typically used in a Jupyter Notebook or other similar environments to install three Python libraries:\n",
        "\n",
        "1. `beautifulsoup4`: BeautifulSoup is a library used to scrape data from HTML and XML files. It creates a parse tree that can be used to extract data easily.\n",
        "\n",
        "2. `requests`: This is a popular library for sending HTTP requests in Python. It abstracts the complexities of making requests behind a simple API, allowing you to send HTTP/1.1 requests.\n",
        "\n",
        "3. `nltk`: NLTK (Natural Language Toolkit) is a library for working with human language data (text). It provides easy-to-use interfaces to over 50 corpora and lexical resources, such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and more.\n",
        "\n",
        "The `!` at the beginning of the line tells the Jupyter Notebook to execute the command in the system shell, allowing you to run terminal commands from within the notebook.\n",
        "\n",
        "So, when this command is executed, the system's package manager for Python (pip) will install these three libraries, making them available for use in your Python code within that environment."
      ],
      "metadata": {
        "id": "t-rRjgHs-vAc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3Ezh3pP8Ka9-",
        "outputId": "8a32b4fb-0235-426b-8b8c-95b288614c7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's what each part of the code does:\n",
        "\n",
        "1. `from collections import Counter`: This line imports the `Counter` class from the `collections` module. `Counter` is a container that keeps track of how many times equivalent values are added. It can be used to count the occurrences of items in a list, for example.\n",
        "\n",
        "2. `import nltk`: This line imports the Natural Language Toolkit (NLTK) module, a widely-used library for working with human language data.\n",
        "\n",
        "3. `from nltk.tokenize import word_tokenize`: This imports the `word_tokenize` function from the `nltk.tokenize` module. This function is used to split a text into a list of individual words, commonly referred to as tokenization in natural language processing.\n",
        "\n",
        "4. `nltk.download('punkt')`: This line downloads the Punkt tokenizer models. NLTK uses these models for the `word_tokenize` function to work properly. The Punkt tokenizer divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences. If the Punkt tokenizer models are not already downloaded, you'll need to do so to tokenize text using NLTK's `word_tokenize` method.\n",
        "\n",
        "In summary, this code sets up the environment for natural language processing tasks by importing necessary libraries and downloading required resources. You would typically see this code at the beginning of a script or notebook where text analysis or other natural language processing is being performed."
      ],
      "metadata": {
        "id": "rLEZfZMw_AJm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EOpoWCSrZUDh"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from torchtext.vocab import Vocab\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet includes import statements for various libraries that would typically be used for web scraping and natural language processing (NLP), possibly in combination with deep learning. Here's a breakdown:\n",
        "\n",
        "- `import requests`: Imports the Requests library, used for making HTTP requests in Python.\n",
        "- `from bs4 import BeautifulSoup`: Imports BeautifulSoup from bs4, used for parsing and extracting data from HTML/XML documents.\n",
        "- `from nltk.tokenize import word_tokenize`: Imports the word_tokenize function from NLTK, used for splitting text into words.\n",
        "- `from collections import Counter`: Imports the Counter class from collections, used for counting hashable objects.\n",
        "- `from torchtext.vocab import Vocab`: Imports the Vocab class from torchtext, used for mapping words to indices.\n",
        "- `from torch.utils.data import Dataset`: Imports the Dataset class from torch.utils.data, a base class for creating custom datasets in PyTorch.\n",
        "- `import torch`: Imports the main PyTorch library, used for building and training neural networks.\n",
        "- `import torch.nn as nn`: Imports the neural network module from PyTorch, providing classes for building networks.\n",
        "- `from torch.utils.data import DataLoader`: Imports DataLoader from torch.utils.data, used for loading datasets in iterable mini-batches.\n",
        "- `import time`: Imports the time module, used for time-related tasks, like measuring durations.\n",
        "- `import torch.nn.functional as F`: Imports the functional module from torch.nn, providing functions like activation and loss functions."
      ],
      "metadata": {
        "id": "UZ9VLu2k_luj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgXsHkmyQFB2"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LcEiqjVQMkZB",
        "outputId": "3045cce3-c7cd-4048-f6b5-910a5d8d571a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total vocab scraped: 15,935\n",
            "Total tokens scraped: 152,492\n",
            "Number of batches: 4750\n",
            "cuda\n",
            "The encoder has 18,512,384 trainable parameters.\n",
            "The decoder has 33,903,408 trainable parameters.\n",
            "The total model has 52,415,792 trainable parameters.\n",
            "Epoch: 0,Batch: 1, Loss: 10.473468780517578\n",
            "Epoch: 0,Batch: 11, Loss: 6.872862339019775\n",
            "Epoch: 0,Batch: 21, Loss: 6.7347612380981445\n",
            "Epoch: 0,Batch: 31, Loss: 6.237522125244141\n",
            "Epoch: 0,Batch: 41, Loss: 6.106812477111816\n",
            "Epoch: 0,Batch: 51, Loss: 5.743758201599121\n",
            "Epoch: 0,Batch: 61, Loss: 5.536795616149902\n",
            "Epoch: 0,Batch: 71, Loss: 5.391904354095459\n",
            "Epoch: 0,Batch: 81, Loss: 5.099989891052246\n",
            "Epoch: 0,Batch: 91, Loss: 5.023227691650391\n",
            "Epoch: 0,Batch: 101, Loss: 4.902891635894775\n",
            "Epoch: 0,Batch: 111, Loss: 4.82517671585083\n",
            "Epoch: 0,Batch: 121, Loss: 4.580105781555176\n",
            "Epoch: 0,Batch: 131, Loss: 4.521768093109131\n",
            "Epoch: 0,Batch: 141, Loss: 4.44677734375\n",
            "Epoch: 0,Batch: 151, Loss: 4.334126949310303\n",
            "Epoch: 0,Batch: 161, Loss: 4.315503120422363\n",
            "Epoch: 0,Batch: 171, Loss: 4.287035942077637\n",
            "Epoch: 0,Batch: 181, Loss: 4.166302680969238\n",
            "Epoch: 0,Batch: 191, Loss: 4.115898132324219\n",
            "Raw logits: tensor([[-4.6330, -3.9143,  3.0516,  ..., -5.5649,  5.5479, -4.9453],\n",
            "        [-4.0680, -2.6590,  3.1031,  ..., -5.0710,  4.7031, -4.1294],\n",
            "        [-3.4671, -2.4803,  4.1015,  ..., -6.2820,  4.7586, -4.4143],\n",
            "        ...,\n",
            "        [-2.2111, -2.0394,  0.0501,  ..., -4.6413,  6.8803, -2.8266],\n",
            "        [-1.5637, -3.6488,  2.8848,  ..., -3.7419,  4.5886, -5.4692],\n",
            "        [-3.0968, -2.6860,  1.9538,  ..., -4.8947,  5.4283, -4.5790]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "Epoch: 0, Loss: 3.9981799125671387\n",
            "Epoch: 1,Batch: 1, Loss: 4.07659387588501\n",
            "Epoch: 1,Batch: 11, Loss: 4.0308732986450195\n",
            "Epoch: 1,Batch: 21, Loss: 3.9692800045013428\n",
            "Raw logits: tensor([[-2.4110, -2.1589,  1.3319,  ..., -4.5373,  6.6541, -2.7746],\n",
            "        [-1.8047, -3.0277, -0.3048,  ..., -3.4986,  4.4285, -3.7581],\n",
            "        [-2.5861, -2.4455,  2.4209,  ..., -4.7534,  6.1526, -5.0285],\n",
            "        ...,\n",
            "        [-3.5967, -3.5605,  4.1390,  ..., -5.1960,  5.5525, -5.8633],\n",
            "        [-2.7599, -3.9438,  2.6069,  ..., -4.6754,  4.8739, -3.8722],\n",
            "        [-2.2939, -1.6357,  0.9714,  ..., -4.0062,  6.6629, -2.4664]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "Epoch: 1, Loss: 3.9692800045013428\n",
            "Epoch: 2,Batch: 1, Loss: 4.01513671875\n",
            "Raw logits: tensor([[-2.5573, -3.7945,  2.3879,  ..., -5.0561,  5.9658, -3.8582],\n",
            "        [-4.0452, -2.5344,  4.1024,  ..., -5.3762,  5.0040, -4.0254],\n",
            "        [-1.3966, -3.7264,  5.3740,  ..., -4.9737,  5.9098, -5.7201],\n",
            "        ...,\n",
            "        [-2.2985, -2.4663,  3.0536,  ..., -4.0740,  5.3802, -3.8964],\n",
            "        [-2.8665, -1.6273,  1.9902,  ..., -3.1625,  6.3503, -4.2289],\n",
            "        [-2.6822, -2.6520,  2.2170,  ..., -6.1256,  4.3575, -4.0054]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "Epoch: 2, Loss: 3.9900457859039307\n",
            "Epoch: 3,Batch: 1, Loss: 4.0528244972229\n",
            "Raw logits: tensor([[-2.7900, -3.1201,  3.7196,  ..., -5.2565,  5.6213, -5.7974],\n",
            "        [-2.2467, -1.5787,  1.5925,  ..., -2.8928,  6.3714, -4.1520],\n",
            "        [-2.8724, -2.9205,  1.4500,  ..., -5.2412,  5.2240, -3.9223],\n",
            "        ...,\n",
            "        [-2.4684, -3.7895,  3.0901,  ..., -5.1209,  6.2648, -3.8935],\n",
            "        [-2.1789, -1.3278,  1.6361,  ..., -3.0918,  5.8447, -3.4350],\n",
            "        [-3.2383, -1.9008,  0.3699,  ..., -4.7904,  6.9069, -3.4219]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "Epoch: 3, Loss: 3.956598997116089\n",
            "Epoch: 4,Batch: 1, Loss: 4.0209174156188965\n",
            "Raw logits: tensor([[-4.0823, -3.2508,  2.8888,  ..., -5.2300,  4.4774, -4.6639],\n",
            "        [-3.5303, -1.8627,  4.2430,  ..., -4.3798,  4.9218, -3.3410],\n",
            "        [-4.2185, -2.3426,  4.3204,  ..., -4.9438,  5.3425, -4.7655],\n",
            "        ...,\n",
            "        [-3.6821, -3.7235,  1.5055,  ..., -4.5648,  5.3460, -3.7326],\n",
            "        [-3.4129, -2.1428,  4.2333,  ..., -4.7194,  5.2625, -5.1443],\n",
            "        [-2.1005, -2.4764,  2.5930,  ..., -4.5118,  3.8053, -1.8298]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "Epoch: 4, Loss: 3.978193521499634\n",
            "Epoch: 5,Batch: 1, Loss: 3.960085868835449\n",
            "Raw logits: tensor([[-1.9799, -3.5085,  2.7200,  ..., -4.7216,  6.4098, -3.9254],\n",
            "        [-1.0779, -2.5394,  4.2462,  ..., -4.6730,  5.2682, -5.0249],\n",
            "        [-2.1662, -2.9732,  2.2002,  ..., -5.1566,  5.7420, -3.0945],\n",
            "        ...,\n",
            "        [-1.0097, -4.3583,  0.9895,  ..., -5.3389,  5.0918, -3.3661],\n",
            "        [-2.5564, -3.2335,  2.3881,  ..., -5.0266,  5.9736, -4.6695],\n",
            "        [-2.0675, -2.9058,  2.7376,  ..., -5.8755,  6.1107, -4.5003]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "Epoch: 5, Loss: 3.960085868835449\n",
            "Epoch: 6,Batch: 1, Loss: 3.956531524658203\n",
            "Raw logits: tensor([[-2.2058, -2.3664, -0.3364,  ..., -4.3249,  5.4112, -4.1427],\n",
            "        [-3.3689, -3.7434,  3.7849,  ..., -4.5838,  5.3842, -5.3898],\n",
            "        [-3.4194, -2.9811,  3.6252,  ..., -5.2422,  5.0449, -5.6937],\n",
            "        ...,\n",
            "        [-4.0835, -2.6044,  1.4882,  ..., -4.7288,  4.8260, -3.3022],\n",
            "        [-2.5884, -2.2250,  3.3127,  ..., -2.3714,  3.6576, -2.8944],\n",
            "        [-2.7444, -2.1808,  4.1001,  ..., -5.4366,  4.6539, -4.1394]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "Epoch: 6, Loss: 3.956531524658203\n",
            "Epoch: 7,Batch: 1, Loss: 4.0114617347717285\n",
            "Raw logits: tensor([[-4.4491, -2.8067,  3.9497,  ..., -5.5498,  5.3289, -4.0153],\n",
            "        [-1.5882, -2.9640,  3.1712,  ..., -5.2587,  3.4617, -3.9332],\n",
            "        [-2.3495, -2.3613, -0.2724,  ..., -4.1254,  5.5034, -4.3396],\n",
            "        ...,\n",
            "        [-2.1050, -2.1008, -1.0841,  ..., -4.0896,  5.2341, -3.9203],\n",
            "        [-2.5541, -2.6972,  4.3245,  ..., -5.6296,  5.6881, -5.8868],\n",
            "        [-3.9002, -3.1566,  3.4707,  ..., -5.2139,  4.9680, -5.8781]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "Epoch: 7, Loss: 3.962952136993408\n",
            "Epoch: 8,Batch: 1, Loss: 3.990334987640381\n",
            "Raw logits: tensor([[-1.8465, -2.6589,  2.4787,  ..., -4.6659,  5.1428, -5.1481],\n",
            "        [-3.3506, -3.2840,  4.4156,  ..., -4.7055,  5.7107, -4.4400],\n",
            "        [-3.8384, -3.4894,  3.2358,  ..., -5.0701,  4.7703, -4.5697],\n",
            "        ...,\n",
            "        [-2.5551, -4.0500,  3.5137,  ..., -4.5762,  5.2144, -4.1980],\n",
            "        [-3.7131, -3.0726,  3.9958,  ..., -4.9607,  5.0119, -3.8567],\n",
            "        [-2.0239, -2.3952,  6.8893,  ..., -4.7924,  4.2392, -3.4516]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "Epoch: 8, Loss: 3.990334987640381\n",
            "Epoch: 9,Batch: 1, Loss: 4.012059688568115\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.83 GiB. GPU 0 has a total capacty of 14.75 GiB of which 749.06 MiB is free. Process 26685 has 14.01 GiB memory in use. Of the allocated memory 10.46 GiB is allocated by PyTorch, and 3.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-1442c88ef874>\u001b[0m in \u001b[0;36m<cell line: 174>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;36m99\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m torch.save({\n",
            "\u001b[0;32m<ipython-input-19-1442c88ef874>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    163\u001b[0m               \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.83 GiB. GPU 0 has a total capacty of 14.75 GiB of which 749.06 MiB is free. Process 26685 has 14.01 GiB memory in use. Of the allocated memory 10.46 GiB is allocated by PyTorch, and 3.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "def create_vocab(text, vocab_size):\n",
        "    tokenized_text = nltk.word_tokenize(text)\n",
        "    word_freq = Counter(tokenized_text)\n",
        "    vocab = {word: i for i, (word, _) in enumerate(word_freq.most_common(vocab_size))}\n",
        "    return vocab\n",
        "\n",
        "def scrape_wikipedia(urls):\n",
        "    text = \"\"\n",
        "    for url in urls:\n",
        "        page = requests.get(url)\n",
        "        soup = BeautifulSoup(page.content, 'html.parser')\n",
        "        paragraphs = soup.find_all('p')\n",
        "        for paragraph in paragraphs:\n",
        "            text += paragraph.get_text()\n",
        "    return text\n",
        "\n",
        "def create_dataset(vocab_size, input_seq_length, text):\n",
        "    dataset = []\n",
        "    tokens = word_tokenize(text)\n",
        "    vocab = {word: i for i, word in enumerate(set(tokens))}\n",
        "    for i in range(0, len(tokens) - input_seq_length):\n",
        "        input_sequence = [vocab[word] for word in tokens[i: i + input_seq_length]]\n",
        "        target = input_sequence[1:] + [vocab_size - 2]\n",
        "        dataset.append((torch.tensor(input_sequence), torch.tensor(target)))\n",
        "    return dataset, vocab, tokens  # returning dataset and vocabulary\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.data = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, h, d_ff, dropout_rate):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.self_attention = nn.MultiheadAttention(d_model, h)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.embedding(input)\n",
        "        x = self.dropout(x)\n",
        "        attn_output, _ = self.self_attention(x, x, x)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.self_attention = nn.MultiheadAttention(d_model, h)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, input, encoder_output, lookahead_mask, padding_mask, training):\n",
        "        x = self.embedding(input)\n",
        "        x = self.dropout(x)\n",
        "        attn_output1, _ = self.self_attention(x, x, x, attn_mask=lookahead_mask, key_padding_mask=padding_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output1))\n",
        "        if encoder_output is not None:\n",
        "            attn_output2, _ = self.self_attention(x, encoder_output, encoder_output)\n",
        "            x = self.norm2(x + self.dropout(attn_output2))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def main():\n",
        "    urls = [\n",
        "    'https://en.wikipedia.org/wiki/American_Revolution',\n",
        "    'https://en.wikipedia.org/wiki/American_Civil_War',\n",
        "    'https://en.wikipedia.org/wiki/World_War_I',\n",
        "    'https://en.wikipedia.org/wiki/World_War_II',\n",
        "    'https://en.wikipedia.org/wiki/Renaissance',\n",
        "    'https://en.wikipedia.org/wiki/Industrial_Revolution',\n",
        "    'https://en.wikipedia.org/wiki/French_Revolution',\n",
        "    'https://en.wikipedia.org/wiki/Ancient_Greece',\n",
        "    'https://en.wikipedia.org/wiki/Roman_Empire',\n",
        "    'https://en.wikipedia.org/wiki/Enlightenment'\n",
        "    ]\n",
        "    vocab_size = 30000\n",
        "    input_seq_length = 512\n",
        "    h = 8\n",
        "    d_k = 64\n",
        "    d_v = 64\n",
        "    d_model = 512\n",
        "    d_ff = 2048\n",
        "    dropout_rate = 0.1\n",
        "    epochs = 20\n",
        "    batch_size = 32\n",
        "    loss_threshold=4\n",
        "    showlogits=1\n",
        "    text = scrape_wikipedia(urls)\n",
        "    raw_dataset, vocab,tokens = create_dataset(vocab_size, input_seq_length, text)\n",
        "    total_words=len(vocab)\n",
        "    total_tokens=len(tokens)\n",
        "    print(f'Total vocab scraped: {total_words:,}')\n",
        "    print(f'Total tokens scraped: {total_tokens:,}')\n",
        "    torch.save(raw_dataset, \"raw_dataset.pt\")\n",
        "    dataset = TextDataset(raw_dataset)\n",
        "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    num_batches = len(data_loader)\n",
        "    print(f'Number of batches: {num_batches}') # input and target sentences\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(device)\n",
        "    encoder = Encoder(vocab_size, d_model, h, d_ff, dropout_rate).to(device)\n",
        "    decoder = Decoder(vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate).to(device)\n",
        "    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.001)\n",
        "\n",
        "    num_parameters_encoder = count_parameters(encoder)\n",
        "    num_parameters_decoder = count_parameters(decoder)\n",
        "\n",
        "    print(f'The encoder has {num_parameters_encoder:,} trainable parameters.')\n",
        "    print(f'The decoder has {num_parameters_decoder:,} trainable parameters.')\n",
        "    total_parameters = num_parameters_encoder + num_parameters_decoder\n",
        "    print(f'The total model has {total_parameters:,} trainable parameters.')\n",
        "\n",
        "    # Start time\n",
        "    start_time = time.time()\n",
        "    with open(\"loss.txt\", \"w\") as f:\n",
        "      for epoch in range(epochs):\n",
        "        for i, (inputs, targets) in enumerate(data_loader):\n",
        "            inputs = inputs.to(device).long()  # Move inputs to device\n",
        "            targets = targets.to(device).long()  # Move targets to device\n",
        "            encoder_output = encoder(inputs)\n",
        "            output = decoder(inputs, encoder_output, None, None, training=True)\n",
        "            output = output.view(-1, output.size(-1))\n",
        "            targets = targets.view(-1)\n",
        "            loss = F.cross_entropy(output, targets)\n",
        "            # Print the batch number and loss every 100 steps\n",
        "            if i % 10 == 0:\n",
        "              print(f\"Epoch: {epoch},Batch: {i+1}, Loss: {loss.item()}\")\n",
        "            # Write the loss to the file\n",
        "            f.write(f\"Epoch: {epoch}, Batch: {i}, Loss: {loss.item()}\\n\")\n",
        "            if loss<loss_threshold :\n",
        "              # Printing the raw logits\n",
        "              print('Raw logits:', output)\n",
        "              break\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Epoch: {}, Loss: {}\".format(epoch, loss.item()))\n",
        "    # End time\n",
        "    end_time = time.time()\n",
        "    print(\"Time taken for training: {} seconds\".format(end_time - start_time))\n",
        "    return encoder, decoder\n",
        "99\n",
        "encoder, decoder = main()\n",
        "\n",
        "torch.save({\n",
        "    \"encoder\": encoder.state_dict(),\n",
        "    \"decoder\": decoder.state_dict()\n",
        "}, \"model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is part of a script that defines functions for scraping text from Wikipedia URLs, creating a dataset, building an Encoder-Decoder architecture using PyTorch, and training the model on the dataset. Here's an overview of each part:\n",
        "\n",
        "### Function: `create_vocab`\n",
        "\n",
        "- **Input**: Text and a vocabulary size\n",
        "- **Output**: A vocabulary that maps the `vocab_size` most common words in the text to unique integers\n",
        "- **Process**:\n",
        "  1. Tokenizes the input text.\n",
        "  2. Counts the frequency of each word.\n",
        "  3. Creates a vocabulary dictionary from the most common words.\n",
        "\n",
        "### Function: `scrape_wikipedia`\n",
        "\n",
        "- **Input**: List of URLs\n",
        "- **Output**: Concatenated text from all the provided URLs\n",
        "- **Process**:\n",
        "  1. Loops through URLs.\n",
        "  2. Sends a GET request to each URL.\n",
        "  3. Parses the HTML content using BeautifulSoup.\n",
        "  4. Concatenates the text from all the `<p>` elements.\n",
        "\n",
        "### Function: `create_dataset`\n",
        "\n",
        "- **Input**: Vocabulary size, input sequence length, and text\n",
        "- **Output**: Dataset, vocabulary, and tokens\n",
        "- **Process**:\n",
        "  1. Tokenizes the text.\n",
        "  2. Creates a vocabulary from the unique tokens.\n",
        "  3. Creates input and target sequences using the vocabulary.\n",
        "  4. Returns the dataset, vocabulary, and tokens.\n",
        "\n",
        "### Classes: `TextDataset`, `Encoder`, and `Decoder`\n",
        "\n",
        "- **TextDataset**: PyTorch dataset class that wraps the dataset.\n",
        "- **Encoder**: Defines an Encoder module with an embedding layer, multi-head self-attention, feed-forward neural network, layer normalization, and dropout.\n",
        "- **Decoder**: Defines a Decoder module, similar to the Encoder, but with an additional output linear layer.\n",
        "\n",
        "### Function: `count_parameters`\n",
        "\n",
        "- **Input**: A PyTorch model\n",
        "- **Output**: Total number of trainable parameters in the model\n",
        "\n",
        "### Function: `main`\n",
        "\n",
        "- **Process**:\n",
        "  1. Defines hyperparameters and URLs to scrape.\n",
        "  2. Scrapes text from Wikipedia using `scrape_wikipedia`.\n",
        "  3. Creates a dataset using `create_dataset`.\n",
        "  4. Initializes the Encoder, Decoder, and Adam optimizer.\n",
        "  5. Loops through epochs and batches, training the model using the created dataset.\n",
        "  6. Prints loss information and can write loss to a file.\n",
        "  7. Saves the trained Encoder and Decoder's state.\n",
        "\n",
        "### Saving and Executing the Model\n",
        "\n",
        "- The trained Encoder and Decoder models are saved to a file `model.pt`.\n",
        "- The main training function is executed by calling `main()`.\n",
        "\n",
        "Overall, this code is building and training an Encoder-Decoder model using scraped text data, a task often used in sequence-to-sequence problems like machine translation or text summarization."
      ],
      "metadata": {
        "id": "1Hq7BIXLAFKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Total sequence pairs = (165,050 - 512)\n",
        "Number of batches = Total sequence pairs / 32\n",
        "                  ≈ 5142"
      ],
      "metadata": {
        "id": "bIchhjDGrab6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code snippet provided is constructing sequences from tokens and then preparing batches from these sequences. Here's a breakdown of what's happening:\n",
        "\n",
        "- You have a total of \\( 165,050 \\) tokens.\n",
        "- You're using sequences of \\( 512 \\) tokens to create input sequences.\n",
        "- For each input sequence, you have an associated target sequence, which is often a shifted version of the input sequence in text generation tasks.\n",
        "- Since you are using sequences of length \\( 512 \\), the last \\( 512 \\) tokens of your text won't have enough subsequent tokens to form a full sequence, so you subtract this value.\n",
        "- The total number of input-target pairs is \\( 165,050 - 512 \\), and these pairs are then divided into batches.\n",
        "\n",
        "The number of batches is then calculated by dividing the total number of input-target pairs by the batch size. The calculation only counts the input sequences, but it implicitly counts the corresponding target sequences since there is a one-to-one correspondence between input sequences and target sequences.\n",
        "\n",
        "In summary, the subtraction of \\( 512 \\) accounts for the fact that you're creating sequences of that length, and it ensures that you don't attempt to create a sequence that would extend beyond the end of your tokens. This subtraction doesn't specifically relate to the distinction between input and target sequences; instead, it relates to the sequence length you are using to construct both input and target sequences."
      ],
      "metadata": {
        "id": "Rx8mORfesfCy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1b5ltBZc1rh"
      },
      "source": [
        "# Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ita3o_pIppX"
      },
      "outputs": [],
      "source": [
        "raw_dataset = torch.load(\"raw_dataset.pt\")\n",
        "dataset = TextDataset(raw_dataset)\n",
        "# Print the first 5 items\n",
        "for i, (input, target) in enumerate(raw_dataset):\n",
        "    print(f\"Input: {input}\")\n",
        "    print(f\"Target: {target}\")\n",
        "    if i >= 4:  # stop after 5 items\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet is loading a previously saved dataset (presumably created earlier in the code you posted) and printing the first five input-target pairs. Here's a breakdown:\n",
        "\n",
        "1. `raw_dataset = torch.load(\"raw_dataset.pt\")`: This line loads a saved dataset from a PyTorch file called \"raw_dataset.pt\". The `raw_dataset` variable is a list of tuples, where each tuple consists of an input tensor and a target tensor.\n",
        "\n",
        "2. `dataset = TextDataset(raw_dataset)`: This line wraps the loaded `raw_dataset` in a `TextDataset` class, which is likely a custom dataset class defined to work with PyTorch's DataLoader. However, this `dataset` variable is not used in the code snippet.\n",
        "\n",
        "3. The loop iterates over the `raw_dataset`, printing the input and target tensors for the first five items:\n",
        "    - `Input: {input}`: This prints the input tensor for each item. Depending on the context of the code, this might represent a sequence of tokenized words (encoded as integers) that form an input sequence.\n",
        "    - `Target: {target}`: This prints the target tensor for each item. Depending on the context, this might represent a sequence of tokenized words that are the expected output for the corresponding input sequence.\n",
        "    - The loop stops after printing the first five items, as controlled by the `if i >= 4:` condition.\n",
        "\n"
      ],
      "metadata": {
        "id": "KSpcTdHmApin"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEmLIMn6c2oN"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsAm5TYhUT7C"
      },
      "outputs": [],
      "source": [
        "# Load the model checkpoint\n",
        "checkpoint = torch.load(\"model.pt\")\n",
        "\n",
        "# Print all keys and values in the checkpoint\n",
        "print(\"Model checkpoint:\")\n",
        "for key, value in checkpoint.items():\n",
        "    print(f\"Key: {key}\")\n",
        "    if isinstance(value, dict):\n",
        "        # If the value is a dictionary (as it is for encoder and decoder state_dicts), print its keys\n",
        "        print(f\"Value keys: {value.keys()}\")\n",
        "    else:\n",
        "        # Otherwise, print the value itself\n",
        "        print(f\"Value: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet is loading a model checkpoint from a file called \"model.pt\" and printing its content. A model checkpoint is a snapshot of the state of a model, typically containing the model's parameters at a particular point during training. Here's what the code is doing:\n",
        "\n",
        "1. **`checkpoint = torch.load(\"model.pt\")`:** This line loads the model checkpoint from the file \"model.pt\". The checkpoint is expected to be a dictionary where the keys identify different parts of the saved state, such as the weights of different components of the model (e.g., encoder and decoder).\n",
        "\n",
        "2. **`print(\"Model checkpoint:\")`:** This prints a string to indicate the beginning of the checkpoint content.\n",
        "\n",
        "3. **Loop through the checkpoint's items:** The loop iterates through each key-value pair in the checkpoint dictionary.\n",
        "\n",
        "    - **`print(f\"Key: {key}\")`:** This prints the key for each item in the checkpoint. This might represent specific components of the model or other information saved in the checkpoint.\n",
        "\n",
        "    - **`if isinstance(value, dict):`:** If the value associated with the key is a dictionary (as it might be for the `state_dict` of the encoder and decoder in a sequence-to-sequence model), it prints the keys of that dictionary using `print(f\"Value keys: {value.keys()}\")`. This could represent the names and organization of the layers and parameters within that part of the model.\n",
        "\n",
        "    - **`else: print(f\"Value: {value}\")`:** If the value is not a dictionary, it prints the value itself. This could include other information saved in the checkpoint, such as training hyperparameters, the state of the optimizer, etc.\n",
        "\n",
        "### What the Outputs Represent:\n",
        "\n",
        "The output from running this code would be a detailed printout of the contents of the model checkpoint:\n",
        "\n",
        "- For components like the encoder and decoder (if they are part of the model), it would print the keys representing the structure of these components.\n",
        "- For other information stored in the checkpoint, it would print the values directly.\n",
        "\n",
        "This is useful for understanding the contents of the checkpoint, inspecting the saved state of the model, and potentially troubleshooting issues with loading the checkpoint for further training or evaluation."
      ],
      "metadata": {
        "id": "2v_wy6d3BNJC"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPvBT453ORvhEw1XnOC5FDr"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}