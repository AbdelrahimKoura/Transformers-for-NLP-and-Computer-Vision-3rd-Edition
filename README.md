# Transformers_3rd_Edition<br>
Part 1 <br>
<br>
Chapter 1: Why Transformers?<br>
<br>
Chapter 2: Getting Started with the Architecture of the Transformer Model<br>				
Multi_Head_Attention_Sub_Layer.ipynb<br>
positional_encoding.ipynb<br>
<br>
Chapter 3: Downstream NLP Tasks with Transformers<br>				
Transformer_tasks.ipynb<br>
<br>
Chapter 4: Machine Translation with the Transformer<br>				
Trax_translation.ipynb<br>
<br>
Chapter 5: Fine-Tuning BERT Models<br>				
BERT_Fine_Tuning_Sentence_Classification_GPU.ipynb<br>
<br>
Chapter 6 : Pretraining a RoBERTa Model from Scratch<br>				
Pretraining a RoBERTa Model from Scratch<br>
KantaiBERT.ipynb
<br>
Part 2
<br>
Chapter 7: The Rise of Suprahuman Transformers with GPT-3 Engines<br>				
Getting_Started_GPT_3.ipynb<br>
Fine_tuning_GPT_3.ipynb<br>
<br>
Chapter 8: Embedding
<br>
Chapter 9 : Matching Tokenizers and Datasets<br>				
Tokenizers.ipynb<br>
Training_OpenAI_GPT_2_CH09.ipynb<br>
<br>
Chapter 10 : Applying Transformers to Legal and Financial Documents for AI Text Summarization<br>				
Summerizing_Text_with_T5.ipynb<br>
(?)Summarizing_with_ChatGPT.ipynb<br>
<br>
Chapter 11 : Semantic Role Labeling<br>				
SRL.ipynb<br>
Semantic_Role_Labeling_with_ChatGPT.ipynb<br>
<br>
Chapter 12 : Let Your Data Do the Talking: Story, Questions, and Answers<br>				
QA.ipynb<br>
01_Basic_QA_Pipeline.ipynb<br>
(x)Haystack_QA_Pipeline.ipynb<br>
<br>
Chapter 13 Detecting Customer Emotions to Make Predictions<br>				
SentimentAnalysis.ipynb<br>
<br>
Chapter 14 : Analyzing Fake News with Transformers<br>				
Fake_News.ipynb<br>
Fake_News_Analysis_with_ChatGPT.ipynb<br>
<br>
Chapter 15 : Interpreting Black Box Transformer Models<br>				
BertViz.ipynb<br>
Understanding_GPT_2_models_with_Ecco.ipynb<br>
<br>
Chapter 16 : The Emergence of Transformer-Driven Copilots<br>				
Domain_Specific_GPT_3_Functionality.ipynb<br>
KantaiBERT_Recommender.ipynb<br>
Vision_Transformer_MLP_Mixer.ipynb<br>
Compact_Convolutional_Transformers.ipynb<br>
<br>
Part III Computer Vision
<br>
Chapter 17: From NLP to Task-Agnostic Transformer Models<br>				
Vision_Transformers.ipynb<br>
DALL_E.ipynb<br>
<br>
Chapter 18: Prompt Art <br>
Chapter 19: Fine-Tuning CV<br>
Chapter 20: Training CV<br>
<br>
Bonus<br>
üê¨Explore and compare ChatGPT, GPT-4 and GPT-3 models<br>				
Exploring_GPT_4_API.ipynb<br>	
<br>
üê¨Create a ChatGPT XAI function that explains ChatGPT and an XAI SHAP function<br>				
XAI_by_ChatGPT_for_ChatGPT.ipynb<br>	
<br>
üê¨Go back to the origins with GPT-2 and ChatGPT<br>				
GPT_2_and_ChatGPT_the_Origins.ipynb<br>	
<br>
üê¨ChatGPT or davinin_instruct? What is best for your project?<br>				
ChatGPT_as_a_Cobot_ChatGPT_versus_davinci_instruct.ipynb<br>	
<br>
üê¨AI Language Model Comparison<br>
-Explore various AI language models and their capabilities through this comprehensive notebook.<br>
-Dive into different APIs and functionalities, such as sentiment analysis, entity recognition, syntax analysis, content classification, and AI vision.<br>
-Discover and compare the offerings of Google Cloud AI Language,Google Cloud AI Vision, OpenAI GPT-4, Google Bard, Microsoft New Bing, ChatGPT Plus-GPT-4,<br> Hugging Face, HuggingGPT, and Google Smart Compose.<br>				
Exploring_and_Comparing_Advanced_AI_Technologies.ipynb<br>
<br>
Chapter 17 :üê¨ Consolidation of Suprahuman Transformers with OpenAI ChatGPT and GPT-4<br>				
Jump_Starting_ChatGPT_with_the_OpenAI_API.ipynb<br>
ChatGPT_Plus_writes_and_explains_classification.ipynb<br>
Getting_Started_with_GPT_4.ipynb<br>
Prompt_Engineering_as_an_alternative_to_fine_tuning.ipynb<br>
Getting_Started_with_the_DALL_E_2_API.ipynb<br>
Speaking_with_ChatGPT.ipynb<br>
ALL_in_One.ipynb<br>
<br>
Archives<br>
<br>
Appendix III: Generic Text Completion with GPT-2<br>				
OpenAI_GPT_2.ipynb<br>
<br>
Appendix IV: Custom Text Completion with GPT-2<br>				
Training_OpenAI_GPT_2.ipynb<br>
<br>
