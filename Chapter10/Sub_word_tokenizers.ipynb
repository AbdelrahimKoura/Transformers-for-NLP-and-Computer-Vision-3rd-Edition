{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPx9F7HkzPZJBxN2EfKh0ss"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Sub_word_tokenizers\n",
        "Copyright 2023, Denis Rothman\n",
        "\n",
        "Sub word tokenizers other than BPE and Wordpiece"
      ],
      "metadata": {
        "id": "2uzUPYCWLnjc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BfR0YFi1Lj5l"
      },
      "outputs": [],
      "source": [
        "!pip install transformers -qq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece -qq"
      ],
      "metadata": {
        "id": "Whr2POZZSV85"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unigram Language Model Tokenization"
      ],
      "metadata": {
        "id": "cYfm1hecNBC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import Unigram\n",
        "from tokenizers.trainers import UnigramTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# Define a sample corpus\n",
        "corpus = [\n",
        "    \"Subword tokenizers break text sequences into subwords.\",\n",
        "    \"This sentence is another part of the corpus.\",\n",
        "    \"Tokenization is the process of breaking text down into smaller units.\",\n",
        "    \"These smaller units can be words, subwords, or even individual characters.\",\n",
        "    \"Transformer models often use subword tokenization.\"\n",
        "]\n",
        "\n",
        "# Instantiate a Unigram tokenizer model\n",
        "tokenizer = Tokenizer(Unigram([]))\n",
        "\n",
        "# Add a pre-tokenizer\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Train the tokenizer model\n",
        "trainer = UnigramTrainer(vocab_size=5000)  # Here you set the desired vocabulary size\n",
        "tokenizer.train_from_iterator(corpus, trainer)\n",
        "\n",
        "# Now let's tokenize the original sentence\n",
        "output = tokenizer.encode(\"Subword tokenizers break text sequences into subwords.\")\n",
        "print(output.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzvI78jzN5BT",
        "outputId": "a28482da-22ae-4909-fdff-ee4b9122eb34"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['S', 'ubword', 'tokeniz', 'er', 's', 'break', 'te', 'x', 't', 'se', 'q', 'u', 'ence', 's', 'in', 'to', 'subword', 's', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SentencePiece tokenization"
      ],
      "metadata": {
        "id": "d9fLr_KeTOIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "import random\n",
        "\n",
        "# Define a basic corpus\n",
        "basic_corpus = [\n",
        "    \"Subword tokenizers break text sequences into subwords.\",\n",
        "    \"This sentence is another part of the corpus.\",\n",
        "    \"Tokenization is the process of breaking text down into smaller units.\",\n",
        "    \"These smaller units can be words, subwords, or even individual characters.\",\n",
        "    \"Transformer models often use subword tokenization.\"\n",
        "]\n",
        "\n",
        "# Generate a larger corpus by repeating sentences from the basic corpus\n",
        "corpus = [random.choice(basic_corpus) for _ in range(10000)]\n",
        "\n",
        "# Write the corpus to a text file\n",
        "with open('large_corpus.txt', 'w') as f:\n",
        "    for sentence in corpus:\n",
        "        f.write(sentence + '\\n')\n",
        "\n",
        "# Train the SentencePiece model\n",
        "spm.SentencePieceTrainer.train(input='large_corpus.txt', model_prefix='m', vocab_size=88)\n",
        "\n",
        "# Load the trained model\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "# Tokenize the original sentence\n",
        "tokens = sp.encode_as_pieces(\"Subword tokenizers break text sequences into subwords.\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__cUMfsNTtgu",
        "outputId": "a71f6776-199c-41a8-dcee-65ae5b9ff678"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁', 'S', 'ubword', '▁tokeniz', 'ers', '▁break', '▁', 'te', 'x', 't', '▁se', 'q', 'u', 'ence', 's', '▁in', 'to', '▁subwords', '.']\n"
          ]
        }
      ]
    }
  ]
}
